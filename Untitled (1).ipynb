{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle-OpenStreetMap-Data\n",
    "\n",
    "On the particular project, I am using data mungling techniques to assess the quality of OpenStreetMapâ€™s (OSM) data for the mumbai city regarding their consistency and uniformity. The data wrangling takes place programmatically, using Python for the most of the process and SQL for items that need further attention.\n",
    "\n",
    "The dataset describes the city of mumbai.Mumbai,India is the closest thing I have to a hometown in the India as I lived there for a good chunk of my childhood, so I was keen to take a look at it in this new, OpenStreetMap-based lens. The size of the dataset is 66 MB and can can be downloaded from here: https://mapzen.com/data/metro-extracts/metro/mumbai_india/\n",
    "\n",
    "About the project\n",
    "\n",
    "Scope\n",
    "\n",
    "OpenStreetMap (OSM) is a collaborative project to create a free editable map of the world. The creation and growth of OSM have been motivated by restrictions on use or availability of map information across much of the world, and the advent of inexpensive portable satellite navigation devices.\n",
    "\n",
    "On the specific project, I am using data from https://www.openstreetmap.org/node/16173235 and data mungling techniques, to assess the quality of their validity, accuracy, completeness, consistency and uniformity. The biggest part of the wrangling takes place programmatically using Python and then the dataset is entered into a SQL database for further examination of any remaining elements that need attention. Finally, I perform some basic exploration and express some ideas for additional improvements.\n",
    "\n",
    "Skills demonstrated\n",
    "\n",
    "Assessment of the quality of data for validity, accuracy, completeness, consistency and uniformity. Parsing and gathering data from popular file formats such as .xml and .csv. Processing data from very large files that cannot be cleaned with spreadsheet programs. Storing, querying, and aggregating data using SQL. Mumbai, India\n",
    "\n",
    "https://www.openstreetmap.org/node/16173235 https://mapzen.com/data/metro-extracts/metro/mumbai_india/\n",
    "\n",
    "Problems Encountered in the Map\n",
    "\n",
    "Problems Encountered in the Map Once the location was decided, I downloaded the full extract of the region and ran Python code to investigate any issues with the data. The following problems were discovered:\n",
    "\n",
    "Street Names: Incomplete ('hanuman raod ___') or incorrect names ('Zhopadpatti'), along with street abbreviations ('rd.' instead of 'Road')\n",
    "\n",
    "Postal Codes: Inconsistent postal code formats ('500023' and '120045') and incorrect post codes ('123')\n",
    "\n",
    "To tackle these issues, I had to create python scripts to clean each respective category of data. Auditing part is explained in Openstreetmap.ipynb notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created mumbai_sample file which is part of mumbai_india file and it can be used for various experiments before using those on main osm file, we can also get the idea about the format of osm file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"mumbai_india.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"mumbai_sample.osm\"\n",
    "\n",
    "k = 500 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of tags mumbai_india file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 13085,\n",
      " 'nd': 2356083,\n",
      " 'node': 2051208,\n",
      " 'osm': 1,\n",
      " 'relation': 3994,\n",
      " 'tag': 391731,\n",
      " 'way': 283963}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "tags={}\n",
    "def count_tags(sample_file):\n",
    "    for event, elem in ET.iterparse(sample_file, events=(\"start\",)):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "        #print tags\n",
    "    return tags\n",
    "tags = count_tags('mumbai_india.osm')\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag types \n",
    "\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \n",
    "  \"problemchars\", for tags with problematic characters, \n",
    "  \n",
    "  \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 375605, 'lower_colon': 15589, 'other': 530, 'problemchars': 7}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "            if lower.search(element.attrib['k']):\n",
    "              keys[\"lower\"] += 1\n",
    "            elif lower_colon.search(element.attrib['k']):\n",
    "              keys[\"lower_colon\"] += 1\n",
    "            elif problemchars.search(element.attrib['k']):\n",
    "              keys[\"problemchars\"] += 1\n",
    "            else:\n",
    "              keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "keys = process_map('mumbai_india.osm')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find number of unique users in mumbai_india osm file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "def get_user(element):\n",
    "     if \"uid\" in element.attrib:\n",
    "        return element.attrib[\"uid\"]\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "       users.add(get_user(element))\n",
    "       users.discard(None)\n",
    "    return users\n",
    "users = process_map('mumbai_india.osm')\n",
    "#pprint.pprint(users)\n",
    "len(users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Names\n",
    "There are two things the auditing function needed to accomplish:\n",
    "Replace any abbreviations of street types with the type completely spelled out.\n",
    "  1. 'Veer Savarkar Rd' -> 'Veer Savarkar Road'\n",
    "  \n",
    "Replace incorrect or incomplete street names with the corrected/complete counterparts.\n",
    "\n",
    "  2. 'jhopadpatti' -> 'Slums'\n",
    "\n",
    "In order to discover problematic street names, I first had to use the regular expression (re) module to locate street types at the end of an address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = \"mumbai_india.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, I matched them against a list of acceptable street types. If they weren't in the list of expected types, they would be added to a dictionary as keys, with the addresses that contain the problematic cases as the values.\n",
    "\n",
    "Having this overview allowed me to determine what my auditing function needed to accomplish. I created a dictionaries for mapping/correcting purposes - 'mapping'. If my function came across a problematic street type, it would refer to that dictionaries for the corrected version to be replaced with.\n",
    "\n",
    "An analysis of the XML data, along with outside research on Google Maps and OpenStreetMaps, was needed to identify the missing street types for the incomplete street names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"mumbai_india.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#expected = [\"Slums\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"Jhopadpatti\": \"Slums\",\n",
    "            \"Marg,\": \"Marg\",\n",
    "            \"Mumbai,\":\"Mumbai\",\n",
    "            \"ROad\":\"Road\",\n",
    "            \"Rd\":\"Road\",\n",
    "            \"Raod\":\"Road\",\n",
    "            \"Rd.\":\"Road\",\n",
    "            \"road\":\"Road\"\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        #if street_type not in expected:\n",
    "        street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    #print '\\nSTART'\n",
    "    #print name\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() in mapping.keys():        \n",
    "        name=re.sub(street_type_re,mapping[m.group()],name)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return name\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "#pprint.pprint(dict(st_types))\n",
    "\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        #print name, \"=>\", better_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Postal Codes\n",
    "\n",
    "Postal codes of Mumbai all start with '40' and all are of 6 digits therefore for auditing postcodes it was important to ensure the postal codes are in the 6-digit format\n",
    "To locate and extract the 6-digit zip code in cases where there are irrelevant or too much information - i.e. random white spaces or an additional strings after the zip code.\n",
    "Example conversions:\n",
    "1. 400076,India\n",
    "\n",
    "To identify problems with postal codes, I used a function that would search through the XML data for every instance of a postal code, and add it to a set of unique postal codes if it satisfies above mentioned conditions.\n",
    "The audit function can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"mumbai_india.osm\"\n",
    "tree = ET.parse(OSMFILE)\n",
    "root = tree.getroot()\n",
    "\n",
    "postal_type_re = re.compile('^(4)(0)\\d{4}$')\n",
    "\n",
    "\n",
    "\n",
    "def audit_postal_code(postal_types,postal_value):\n",
    "    m = postal_type_re.search(postal_value)\n",
    "    if m:\n",
    "        postal_type=m.group()\n",
    "        postal_types[postal_type].add(postal_value)\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    postal_types=defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag in[\"node\", \"way\", \"relation\"] :\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_types,tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return postal_types\n",
    "\n",
    "\n",
    "#audit(SAMPLE_FILE)\n",
    "#audit(OSMFILE)\n",
    "#pprint.pprint(dict(audit(OSMFILE)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below shows the bad postal codes which are either not of mumbai or are in incorrect format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['421501', '421503', '410206', '410210', '410210', '410210', '410210', '412108', '410701', '421202', '421202', '4000607', '40058', '421501', '63103', '421501', '421501', '410201', '410201', '410201', '4000082', '400059.', '410210', '110092', '123', '410209', '410209', '410209', '421302', '410206', '4000072', '410206', '421301', '421301', '4000072', '421306', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410208', '421501', '410206', '40076', '560023', '590006', '492006', '410210', '410206', '560092', '410206', '410206', '410206', '410206', '410206', '410206', '410206', '410206', '410206', '410221', '410102', '410206', '421601', '410206', '421005', '410206', '421501', '410221', '410206', '410206', '48147', '410218', '421201', '421302', '410206', '412108', '412108', '421501', '421501', '410206', '421311', '400076India', '400076India', '410210', '40081', '410210', '410201', '410201', '40001', '40049', '421501', '410406', '412108', '400076India', '421302', '500053', '421201', '410206', '43', '40051', '400076India', '421202', '410206', '410210', '421302', '560103', '500054', '410210', '410210', '410210', '410210', '410218', '421301', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410206', '410206', '410210', '410210', '421301', '410210', '421002', '421203', '410206', '410209', '421306', '421202', '421202', '421503', '410401', '410210', '410210', '410210', '410210', '410210', '410210', '410210', '410208', '410208', '410209', '410210', '410206', '412108', '421004', '410206', '421201']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "tree = ET.parse(OSMFILE)\n",
    "root = tree.getroot()\n",
    "\n",
    "counttotal = 0\n",
    "count = 0\n",
    "wp = []\n",
    "regex = re.compile('^(4)(0)\\d{4}$')\n",
    "for i in tree.getiterator('tag'):\n",
    "    k1 = i.get(\"k\")\n",
    "    if k1 == \"addr:postcode\":\n",
    "        v1 = i.get(\"v\")\n",
    "        m1 = regex.match(v1)\n",
    "        if not m1:\n",
    "            counttotal = counttotal +1\n",
    "            if len(v1) <> 6:\n",
    "                v1 = v1.replace(\" \",\"\")\n",
    "                v1 = v1.replace(\",\",\"\")\n",
    "                v1 = v1.replace(\"-\",\"\")\n",
    "                v1 = v1.replace('\"',\"\")\n",
    "                v1 = v1.replace('55',\"5\")\n",
    "                m2 = regex.match(v1)\n",
    "                if not m2:\n",
    "                    wp.append(v1)\n",
    "                    count = count +1\n",
    "            elif len(v1) == 6:\n",
    "                wp.append(v1)\n",
    "                count = count +1\n",
    "print wp\n",
    "count\n",
    "#print counttotal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is use to update the postal codes by which all postal codes from bad postals mentioned above will be replaced by '000000'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update_postcode(postcode):\n",
    "    search = re.search(r'^\\D*(\\d{6}).*',postcode)\n",
    "    search1 = re.search('^(4)(0)\\d{4}$',postcode)\n",
    "    if search1:\n",
    "        return search.group(1) \n",
    "    else :\n",
    "        return '000000'\n",
    "for i in tree.getiterator('tag'):\n",
    "    k1 = i.get(\"k\")\n",
    "    if k1 == \"addr:postcode\":\n",
    "        v1 = i.get(\"v\")\n",
    "        update_postcode(v1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the XML File and Writing the Data to a CSV\n",
    "\n",
    "In order to begin moving our data towards a SQL database for analysis, I first had to parse the XML file and transform it from a document format to a tabular format. I was then able to write the data across multiple (5) .csv files, allowing us to easily import it to a SQL database. It is in this script where the data cleaning functions are applied.\n",
    "\n",
    "Below is the data. In this code portion of the shape_element function, handles the bulk of the format shaping and utilizes the audit functions I displayed above to clean the data as well. This portion focuses on shaping way_tags and nodes_tags from the XML file into a Python dictionary.Process_map function is used to write data into csv file.In shape_element function update_postcode and update_name functions are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"mumbai_india.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "  \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_dict= {}\n",
    "            # Calling the cleaning function\n",
    "            # replace name with the value of the attribute\n",
    "            if tag.attrib['k'] == 'addr:street':\n",
    "                tag.attrib['v'] = update_name(tag.attrib['v'], mapping)\n",
    "            if tag.attrib[\"k\"] == 'addr:postcode':\n",
    "                tag.attrib[\"v\"] = update_postcode(tag.attrib[\"v\"])\n",
    "            \n",
    "            tag_dict['id'] = node_attribs['id']\n",
    "            key = tag.attrib['k']\n",
    "            \n",
    "            if re.search(PROBLEMCHARS, tag.attrib['k']):\n",
    "                pass\n",
    "            if re.search(LOWER_COLON, tag.attrib['k']):\n",
    "                pass\n",
    "            if ':' in tag.attrib['k']:\n",
    "                type = key[: key.index(':')]\n",
    "                key = key[key.index(':')+1 :]   \n",
    "            else:\n",
    "                type = 'regular'   \n",
    "            \n",
    "            tag_dict['key'] = key\n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "            tag_dict['type'] = type\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "            #pprint.pprint(dict(tag_dict))\n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_dict1= {}\n",
    "\n",
    "            if tag.attrib[\"k\"] == 'addr:postcode':\n",
    "                tag.attrib[\"v\"] = update_postcode(tag.attrib[\"v\"])\n",
    "            tag_dict1['id'] = way_attribs['id']\n",
    "            key = tag.attrib['k']\n",
    "            \n",
    "            if re.search(PROBLEMCHARS, tag.attrib['k']):\n",
    "                pass\n",
    "            if re.search(LOWER_COLON, tag.attrib['k']):\n",
    "                pass\n",
    "            if ':' in tag.attrib['k']:\n",
    "                type = key[: key.index(':')]\n",
    "                key = key[key.index(':')+1 :]\n",
    "            else:\n",
    "                type = 'regular' \n",
    "            tag_dict1['key'] = key\n",
    "            tag_dict1['value'] = tag.attrib['v']\n",
    "            tag_dict1['type'] = type\n",
    "            tags.append(tag_dict1) \n",
    "            \n",
    "        i= 0\n",
    "        for tag in element.iter(\"nd\"):\n",
    "            way_dict = {}\n",
    "            way_dict[\"id\"] = way_attribs[\"id\"]\n",
    "            way_dict[\"node_id\"] = tag.attrib[\"ref\"]\n",
    "            way_dict[\"position\"] = i\n",
    "            way_nodes.append(way_dict)\n",
    "            i +=1    \n",
    "        \n",
    "        # pprint.pprint(dict(way_dict)) \n",
    "            \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'wb') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing CSV Files to SQL Database\n",
    "\n",
    "With the CSV files written, I was able to begin the process of importing the data into a SQL database. In my case, I used SQLite 3 and accessed it through notebook to first create database named a Mumbai_india,then add all data from csv files to database.\n",
    "\n",
    "There are two steps I needed to accomplish to complete this task:\n",
    "\n",
    "1. Create five tables on sqlite based on a schema that matches their respective .csv files\n",
    "2. Import each .csv file into the appropriate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, sqlite3\n",
    "from pprint import pprint\n",
    "sql_file=\"Mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes;''')\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes(id INTEGER, lat REAL, lon REAL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp DATE);\") # use your column names here\n",
    "con.commit()\n",
    "with open('nodes.csv','rb') as thr: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(thr) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['lat'].decode(\"utf-8\"),i['lon'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#con.close()\n",
    "\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags;''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE Nodes_tags (id INTEGER, key TEXT, value TEXT, type TEXT);\") # use your column names here\n",
    "con.commit()\n",
    "with open('nodes_tags.csv','rb') as thr: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(thr) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO Nodes_tags (id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways;''')\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways (id INTEGER, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp DATE);\") # use your column names here\n",
    "con.commit()\n",
    "with open('ways.csv','rb') as thr: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(thr) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"),i['version'].decode(\"utf-8\"),i['changeset'].decode(\"utf-8\"),i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id , user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags;''')\n",
    "con.commit()\n",
    "\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_tags (id INTEGER, key TEXT, value TEXT, type TEXT);\") # use your column names here\n",
    "con.commit()\n",
    "with open('ways_tags.csv','rb') as thr: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(thr) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes;''')\n",
    "con.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id INTEGER, node_id INTEGER, position INTEGER);\") # use your column names here\n",
    "con.commit()\n",
    "with open('ways_nodes.csv','rb') as thr: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(thr) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"),i['node_id'].decode(\"utf-8\"),i['position'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?,?,?);\", to_db)\n",
    "con.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "In this section, I'll execute a number of SQL queries in order to analyze the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users with most numbers of contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'parambyte', 69327),\n",
      " (u'PlaneMad', 68363),\n",
      " (u'anushap', 62553),\n",
      " (u'Ashok09', 62208),\n",
      " (u'Narsimulu', 55611),\n",
      " (u'Srikanth07', 53795),\n",
      " (u'premkumar', 51097)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT user,count(*) FROM nodes GROUP BY user ORDER BY count(*) DESC LIMIT 7')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'building', 223631),\n",
      " (u'highway', 40437),\n",
      " (u'name', 11741),\n",
      " (u'oneway', 4466),\n",
      " (u'source', 4142),\n",
      " (u'landuse', 3038),\n",
      " (u'levels', 2673)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT key,count(*) FROM ways_tags GROUP BY key ORDER BY count(*) DESC LIMIT 7')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of unique users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1739,)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of users who contributed for less than 8 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1053,)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT COUNT(*) FROM (SELECT e.user, COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) as e GROUP BY e.user HAVING num<=8)  u')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of cuisines available in mumbai and number of restaurants it is available in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'indian', 66),\n",
      " (u'regional', 21),\n",
      " (u'pizza', 14),\n",
      " (u'vegetarian', 13),\n",
      " (u'chinese', 12),\n",
      " (u'italian', 11),\n",
      " (u'burger', 5),\n",
      " (u'international', 5),\n",
      " (u'seafood', 4),\n",
      " (u'asian', 3),\n",
      " (u'South_Indian', 2),\n",
      " (u'all_types_of_food', 2),\n",
      " (u'lebanese', 2),\n",
      " (u'Indian,_Chinese_etc', 1),\n",
      " (u'Seafood', 1),\n",
      " (u'Vegetarian_Restaurant', 1),\n",
      " (u'american', 1),\n",
      " (u'cafe', 1),\n",
      " (u'chicken;fish;indian', 1),\n",
      " (u'chicken;kebab;indian', 1),\n",
      " (u'chicken_,fish,cafe', 1),\n",
      " (u'fast_food', 1),\n",
      " (u'grill;coffee_shop;asian;noodles;fish_and_chips;diner;chicken;italian_pizza;indian;curry;fish;french;friture;chinese;barbecue',\n",
      "  1),\n",
      " (u'indian;south_indian', 1),\n",
      " (u'indian_aagri', 1),\n",
      " (u'italian_pizza;pizza', 1),\n",
      " (u'lebanese,_chinese,_indian', 1),\n",
      " (u'local', 1),\n",
      " (u'mediterranean', 1),\n",
      " (u'only_vegiterian', 1),\n",
      " (u'oriental', 1),\n",
      " (u'persian', 1),\n",
      " (u'sad_food', 1),\n",
      " (u'south Indian; Punjabi; agari; malwani; Chinese', 1),\n",
      " (u'south_indian', 1),\n",
      " (u'south_indian,_chinese', 1),\n",
      " (u'spanish', 1),\n",
      " (u'sweets_shop', 1),\n",
      " (u'thai', 1),\n",
      " (u'vegan;ice_cream;italian_pizza;indian;vegetarian;local', 1),\n",
      " (u'vegetarian;indian', 1)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"restaurant\") as i ON nodes_tags.id=i.id WHERE nodes_tags.key=\"cuisine\" GROUP BY nodes_tags.value ORDER BY num DESC')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different religions and number of places where they are workshipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hindu', 125),\n",
      " (u'muslim', 71),\n",
      " (u'christian', 34),\n",
      " (u'buddhist', 13),\n",
      " (u'jain', 6),\n",
      " (u'sikh', 4),\n",
      " (u'zoroastrian', 2),\n",
      " (u'jewish', 1)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags  JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"place_of_worship\") as i ON nodes_tags.id=i.id WHERE nodes_tags.key=\"religion\" GROUP BY nodes_tags.value ORDER BY num DESC')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different leisures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'park', 63),\n",
      " (u'playground', 20),\n",
      " (u'sports_centre', 15),\n",
      " (u'garden', 10),\n",
      " (u'fitness_centre', 8),\n",
      " (u'pitch', 6),\n",
      " (u'swimming_pool', 4),\n",
      " (u'aquarium', 1),\n",
      " (u'fitness_station', 1),\n",
      " (u'golf_course', 1),\n",
      " (u'stadium', 1)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT nodes_tags.value, count(*) as num FROM nodes_tags  WHERE nodes_tags.key=\"leisure\" GROUP BY nodes_tags.value ORDER BY num DESC')\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names of few corrected streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Hanuman Road', 78),\n",
      " (u'Yashavant Nagar Road', 29),\n",
      " (u'Hiranandani Estate', 24),\n",
      " (u'P.L. Lokhande Marg', 24),\n",
      " (u'New Link Road, Andheri West', 21),\n",
      " (u'LBS Marg', 18),\n",
      " (u'Road Number 3', 18),\n",
      " (u'Thane Ghodbunder Road', 18),\n",
      " (u'Eastern Express Highway', 14),\n",
      " (u'GD Somani Road', 13)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT value, count(*) as num FROM nodes_tags WHERE key=\"street\" GROUP BY value ORDER BY num DESC LIMIT 10')\n",
    "\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cities surrounding mumbai and number of nodes in those cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Mumbai', 607),\n",
      " (u'Bandra, Mumbai', 566),\n",
      " (u'mumbai', 187),\n",
      " (u'Virar West', 91),\n",
      " (u'Mulund (West)', 79),\n",
      " (u'Navi Mumbai', 70),\n",
      " (u'MUMBAI', 68),\n",
      " (u'Mulund (East)', 62),\n",
      " (u'Thane', 49),\n",
      " (u'Kharghar', 43)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT tags.value, COUNT(*) as count FROM (SELECT * FROM nodes_tags UNION ALL  SELECT * FROM ways_tags) tags WHERE tags.key LIKE \"%city\" GROUP BY tags.value ORDER BY count DESC LIMIT 10')\n",
    "\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different postcodes in ways_tags after using update postcode function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'car_rental', 1),\n",
      " (u'cold_storage', 1),\n",
      " (u'conference_centre', 1),\n",
      " (u'cyber_cafe', 1),\n",
      " (u'electric socket', 1),\n",
      " (u'internet_cafe', 1),\n",
      " (u'meditation_centre', 1),\n",
      " (u'parking_entrance', 1),\n",
      " (u'parking_space', 1),\n",
      " (u'picnic spot', 1)]\n"
     ]
    }
   ],
   "source": [
    "sql_file=\"mumbai_india.db\"\n",
    "con = sqlite3.connect(sql_file)\n",
    "cur = con.cursor()\n",
    "cur.execute('SELECT value, COUNT(*) as count FROM nodes_tags WHERE key=\"amenity\" GROUP BY value ORDER BY count LIMIT 10')\n",
    "\n",
    "all_rows=cur.fetchall()\n",
    "pprint(all_rows)\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Additional Improvements\n",
    "\n",
    "There are several areas of improvement of the project in the future. The first one is on the completeness of the data. All the above analysis is based on a dataset that reflects a big part of mumbai but not only mumbai. The reason for this is the lack of a way to download a dataset for the entire mumbai without including parts of the neighboring cities. The analyst has to either select a part of the island/city or select a wider area that includes parts of thane and ratnagiri. Also, because of relations between nodes, ways, and relations, the downloaded data expand much further than the actual selection.\n",
    "\n",
    "As a future improvement, I would download a wider selection or the metro extract from MapZen and filter the non-mumbai nodes and their references. The initial filtering could take place by introducing some latitude/longitude limits in the code to sort out most of the \"non-m\" nodes.\n",
    "\n",
    "The second area with room for future improvement is the exploratory analysis of the dataset. Just to mention some of the explorings that could take place:\n",
    "\n",
    "\n",
    "1.Popular franchises in the country (fast food, conventional stores, etc.)\n",
    "\n",
    "2.Selection of a bank based on the average distance you have to walk for an ATM.\n",
    "\n",
    "3.Which area has the biggest parks and recreation spaces.\n",
    "\n",
    "The scope of the current project was the wrangling of the dataset, so all the above have been left for future improvement.\n",
    "\n",
    "Increasing Submissions\n",
    "\n",
    "Going through this dataset, my concerns were less with the cleanliness of the data - as I found it surprisingly clean - and more with the lack of data. This part of mumbai is too big to have as little information as it does. I think OpenStreetMap can go a long way in developing their map database if they took on certain initiative to increase engagement with their service. One possible initiative would be for OpenStreetMap to form partnerships with educational institutions such as schools, or maybe libraries, to engage students with their service. As a way to develop computer and internet literacy, computer-related courses can teach students how to use OpenStreetMap. It'll expose them to online maps, GPS technology, how to participate in open source projects, and more - all while adding data to a free resource that could benefit the members of the community and the world.\n",
    "\n",
    "Anticipated Problem: However, the concern here is that you might see an influx of dirty, unreliable data, particularly if the people behind them aren't very computer literate or only participating because it's a mandatory portion of a course. Naturally the data that come from volunteers who get involved because of their genuine passion for the project would be of higher quality.\n",
    "\n",
    "Ensuring Data Consistency\n",
    "\n",
    "For data improvement, the biggest problem I came across my data before I cleaned it was the lack of a unified format for street types or phone numbers, or simply incomplete information. If OpenStreetMap had a hard format that street types, phone numbers, zip codes, etc. should follow - and they ensured the format is appropriate for the city/country - there would be much cleaner data for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It's clear from what we've seen that the mumbai OpenStretMap data is still incomplete and incorrect but there is still much in this city to be found and explored. The upside is that a lot of the data that has been entered is fairly clean, so future OSM users who embark on the task of improving the dataset with new data won't have much to worry about with regards to cleaning prior submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
